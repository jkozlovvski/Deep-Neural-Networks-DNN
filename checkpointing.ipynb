{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E70zdhmHLZbr"
      },
      "source": [
        "# Checkpointing\n",
        "\n",
        "Your task is to implement checkpointing for a MLP using NumPy.\n",
        "\n",
        "You are free to use the implementation of a MLP and the backpropagation algorithm that you have developed during lab sessions.\n",
        "\n",
        "The key takeaway from this task is that with checkpointing we can trade off the computational resources needed to compute the forward pass of the network for the memory requirement needed to perform a backward pass in the network, which is often a major bottleneck when training large networks. In plain english, we can slightly increase the time required for training our network to save some of our GPU's precious memory.\n",
        "\n",
        "## What is checkpointing?\n",
        "\n",
        "The aim of checkpointing is to save every $n$-th layer's (e.g. every 2-nd layer's) forward result (instead of saving every layer's forward result as in plain backpropagation) and use these checkpoints for recomputing the forward pass of the network upon doing a backward pass. Checkpoint layers are kept in memory after the forward pass, while the remaining activations are recomputed at most once. After being recomputed, the non-checkpoint layers are kept in memory until they are no longer required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JP47CpHjLZbw"
      },
      "source": [
        "# What should be done\n",
        "\n",
        "1. Take the implementation a MLP trained with backpropagation. Analyze the algorithm with respect to the memory that is used by the algorithm with respect to the number of hidden layers.\n",
        "\n",
        "2. Implement a class NetworkWithCheckpointing that inherits from the Network class defined during lab sessions by:\n",
        "    a) implementing a method `forward_between_checkpoints` that will recompute the forward pass of the network using one of the checkpointed layers\n",
        "    b) override the method `backprop` to use only checkpointed layers and otherwise compute the activations using `forward_between_checkpoints` method and keep it in memory until no longer needed.\n",
        "\n",
        "3. Train your network with checkpoinintg on MNIST. Compare running times and memory usage with respect to the network without checkpointing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QqiYlsC8LZbw"
      },
      "source": [
        "# Implement Checkpointing for a MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hRnWHwX1LZbx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/juliankozlowski/Library/Python/3.8/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ufPTP84aLZby",
        "outputId": "aca45bea-43b3-4920-96c5-66b7046c01f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-19 11:03:14--  https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "Translacja s3.amazonaws.com (s3.amazonaws.com)... 52.217.18.62, 52.216.207.69, 52.216.108.229, ...\n",
            "Łączenie się z s3.amazonaws.com (s3.amazonaws.com)|52.217.18.62|:443... połączono.\n",
            "Żądanie HTTP wysłano, oczekiwanie na odpowiedź... 200 OK\n",
            "Długość: 11490434 (11M) [application/octet-stream]\n",
            "Zapis do: `mnist.npz'\n",
            "\n",
            "mnist.npz           100%[===================>]  10,96M  7,57MB/s     w 1,4s    \n",
            "\n",
            "2022-11-19 11:03:16 (7,57 MB/s) - zapisano `mnist.npz' [11490434/11490434]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NBKMS9JHLZb0"
      },
      "outputs": [],
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "def load_mnist(path='mnist.npz'):\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utils functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "cmTTADiELZb0"
      },
      "outputs": [],
      "source": [
        "class NetworkNoCheckpoints(object):\n",
        "    def __init__(self, sizes):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        delta_nabla_b, delta_nabla_w = self.backprop(x_mini_batch.T, y_mini_batch.T)\n",
        "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(x_mini_batch))*nw \n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(x_mini_batch))*nb \n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    def backprop(self, X, y):\n",
        "        # Now X is a matrix of inputs\n",
        "        # For a single input (x,y) return a tuple of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "        \n",
        "        # First initialize the list of gradient arrays\n",
        "        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n",
        "        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n",
        "        \n",
        "        # Then go forward remembering all values before and after activations\n",
        "        # in two other array lists\n",
        "        f_ = [np.zeros_like(p) for p in self.sizes]\n",
        "        g_ = [np.zeros_like(p) for p in self.sizes]\n",
        "        f_[0] = g_[0] = X\n",
        "\n",
        "        for layer in range(1, self.num_layers):\n",
        "          f_[layer] = self.weights[layer - 1] @ g_[layer - 1] + self.biases[layer - 1]\n",
        "          g_[layer] = sigmoid(f_[layer])\n",
        "          \n",
        "\n",
        "        # Now go backward from the final cost applying backpropagation\n",
        "\n",
        "        f_derivatives = [np.zeros_like(p) for p in self.sizes]\n",
        "        g_derivatives = [np.zeros_like(p) for p in self.sizes]\n",
        "        N = len(self.sizes) - 1\n",
        "        g_derivatives[N] = self.cost_derivative(g_[N], y)\n",
        "        f_derivatives[N] = g_derivatives[N] * sigmoid(g_[N]) * (1 - sigmoid(g_[N]))        \n",
        "\n",
        "        for layer in reversed(range(self.num_layers - 1)):\n",
        "          g_derivatives[layer] = self.weights[layer].T @ f_derivatives[layer + 1]\n",
        "          f_derivatives[layer] = g_derivatives[layer] * g_[layer] * (1 - g_[layer])\n",
        "          delta_nabla_b[layer] = f_derivatives[layer + 1].sum(axis = 1).reshape(-1, 1)\n",
        "          delta_nabla_w[layer] = f_derivatives[layer + 1] @ g_[layer].T\n",
        "          \n",
        "\n",
        "        return delta_nabla_b, delta_nabla_w\n",
        "\n",
        "    def evaluate(self, x_test_data, y_test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [(np.argmax(self.feedforward(x_test_data[i].reshape(784,1))), np.argmax(y_test_data[i]))\n",
        "                        for i in range(len(x_test_data))]\n",
        "        # return accuracy\n",
        "        return np.mean([int(x == y) for (x, y) in test_results])\n",
        "    \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "    \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        \n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)] \n",
        "                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test)))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network_no_checkpoints = NetworkNoCheckpoints([784, 100, 30, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ref 1.\n",
        "On each backprop iteration we save the whole foward pass in separate array and we use that when going backwards, thus memory usage is linear to the number of layers used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ref 2.\n",
        "Implementing network with checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NetworkWithCheckpointing(NetworkNoCheckpoints):\n",
        "\n",
        "    def __init__(self, sizes, checkpoint_every_nth_layer: int = 0, *args, **kwargs):\n",
        "        super().__init__(sizes, *args, **kwargs)\n",
        "        self.checkpoint_distance = checkpoint_every_nth_layer\n",
        "            \n",
        "    def forward_between_checkpoints(self, g, checkpoint_idx_start, layer_idx_end):\n",
        "        \n",
        "        f_temp = []\n",
        "        g_temp = []\n",
        "        \n",
        "        for layer in range(checkpoint_idx_start, layer_idx_end):\n",
        "            f = self.weights[layer] @ g + self.biases[layer]\n",
        "            f_temp.append(f)\n",
        "            g = sigmoid(f)\n",
        "            g_temp.append(g)\n",
        "            \n",
        "        return f_temp, g_temp\n",
        "\n",
        "    def backprop(self, X, y):\n",
        "        # initialize checkpoints array for checking\n",
        "        checkpoints = np.full(len(self.sizes), False)\n",
        "        for i in range(0, self.num_layers, self.checkpoint_distance):\n",
        "            checkpoints[i] = True\n",
        "        \n",
        "        # initialize all the changes needed\n",
        "        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n",
        "        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n",
        "\n",
        "        # Then go forward remembering only values on checkpoint layers\n",
        "        sizes_checkpoints = np.array(self.sizes)[checkpoints]\n",
        "        f_checkpoints = [np.zeros_like(p) for p in sizes_checkpoints]\n",
        "        g_checkpoints = [np.zeros_like(p) for p in sizes_checkpoints]\n",
        "        f_checkpoints[0] = g_checkpoints[0] = X\n",
        "\n",
        "        # calculating only certain layers\n",
        "        for layer in range(1, len(sizes_checkpoints)):\n",
        "            f_, g_ = self.forward_between_checkpoints(\n",
        "                g = g_checkpoints[layer - 1],\n",
        "                checkpoint_idx_start = (layer - 1) * self.checkpoint_distance, \n",
        "                layer_idx_end = min(self.num_layers, layer * self.checkpoint_distance))\n",
        "            \n",
        "            f_checkpoints[layer], g_checkpoints[layer] = f_[-1], g_[-1]\n",
        "        \n",
        "        # Now go backward from the final cost applying backpropagation\n",
        "        f_derivatives = [np.zeros_like(p) for p in self.sizes]\n",
        "        g_derivatives = [np.zeros_like(p) for p in self.sizes]\n",
        "        N = len(self.sizes) - 1\n",
        "\n",
        "        # If the last layer is checkpoint\n",
        "        if N % self.checkpoint_distance == 0:\n",
        "            g_derivatives[N] = self.cost_derivative(g_checkpoints[-1], y)\n",
        "            f_derivatives[N] = g_derivatives[N] * sigmoid(g_checkpoints[-1]) * (1 - sigmoid(g_checkpoints[-1]))\n",
        "        # If the last layer is not a checkpoint\n",
        "        else:\n",
        "            last_checkpoint_index = N - (N % self.checkpoint_distance)\n",
        "            f_temp, g_temp = self.forward_between_checkpoints(g_checkpoints[-1], last_checkpoint_index, N)\n",
        "            g_derivatives[N] = self.cost_derivative(g_temp[-1], y)\n",
        "            f_derivatives[N] = g_derivatives[N] * sigmoid(g_temp[-1]) * (1 - sigmoid(g_temp[-1]))\n",
        "            for f, g in zip(reversed(f_temp[:-1]), reversed(g_temp[:-1])):\n",
        "                g_derivatives[N - 1] = self.weights[N - 1].T @ f_derivatives[N]\n",
        "                f_derivatives[N - 1] = g_derivatives[N - 1] * g * (1 - g)\n",
        "                delta_nabla_b[N - 1] = f_derivatives[N].sum(axis = 1).reshape(-1, 1)\n",
        "                delta_nabla_w[N - 1] = f_derivatives[N] @ g.T\n",
        "                N = N - 1\n",
        "\n",
        "        for layer in reversed(range(N)):\n",
        "            if layer % self.checkpoint_distance == 0:\n",
        "                g_derivatives[layer] = self.weights[layer].T @ f_derivatives[layer + 1]\n",
        "                f_derivatives[layer] = g_derivatives[layer] * g_checkpoints[layer // self.checkpoint_distance] * (1 - g_checkpoints[layer // self.checkpoint_distance])\n",
        "                delta_nabla_b[layer] = f_derivatives[layer + 1].sum(axis = 1).reshape(-1, 1)\n",
        "                delta_nabla_w[layer] = f_derivatives[layer + 1] @ g_checkpoints[layer // self.checkpoint_distance].T\n",
        "            else:\n",
        "                last_checkpoint_index = layer - (layer % self.checkpoint_distance)\n",
        "                f_temp, g_temp = self.forward_between_checkpoints(g_checkpoints[last_checkpoint_index // self.checkpoint_distance], last_checkpoint_index, layer)\n",
        "                for f, g in zip(reversed(f_temp), reversed(g_temp)):\n",
        "                    g_derivatives[layer] = self.weights[layer].T @ f_derivatives[layer + 1]\n",
        "                    f_derivatives[layer] = g_derivatives[layer] * g * (1 - g)\n",
        "                    delta_nabla_b[layer] = f_derivatives[layer + 1].sum(axis = 1).reshape(-1, 1)\n",
        "                    delta_nabla_w[layer] = f_derivatives[layer + 1] @ g.T\n",
        "                    layer = layer - 1\n",
        "    \n",
        "        return delta_nabla_b, delta_nabla_w\n",
        "\n",
        "        \n",
        "network_with_checkpoints = NetworkWithCheckpointing([784, 100, 30, 10], checkpoint_every_nth_layer=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.8577\n",
            "Epoch: 1, Accuracy: 0.8946\n",
            "Epoch: 2, Accuracy: 0.9118\n",
            "Epoch: 3, Accuracy: 0.9213\n",
            "Epoch: 4, Accuracy: 0.9271\n",
            "Epoch: 5, Accuracy: 0.9311\n",
            "Epoch: 6, Accuracy: 0.9337\n",
            "Epoch: 7, Accuracy: 0.9359\n",
            "Epoch: 8, Accuracy: 0.9381\n",
            "Epoch: 9, Accuracy: 0.9388\n",
            "Epoch: 10, Accuracy: 0.9404\n",
            "Epoch: 11, Accuracy: 0.942\n",
            "Epoch: 12, Accuracy: 0.9429\n",
            "Epoch: 13, Accuracy: 0.9434\n",
            "Epoch: 14, Accuracy: 0.9441\n",
            "Epoch: 15, Accuracy: 0.945\n",
            "Epoch: 16, Accuracy: 0.9459\n",
            "Epoch: 17, Accuracy: 0.9463\n",
            "Epoch: 18, Accuracy: 0.9468\n",
            "Epoch: 19, Accuracy: 0.947\n",
            "Epoch: 20, Accuracy: 0.9473\n",
            "Epoch: 21, Accuracy: 0.9472\n",
            "Epoch: 22, Accuracy: 0.9478\n",
            "Epoch: 23, Accuracy: 0.9481\n",
            "Epoch: 24, Accuracy: 0.9481\n",
            "Epoch: 25, Accuracy: 0.9484\n",
            "Epoch: 26, Accuracy: 0.9482\n",
            "Epoch: 27, Accuracy: 0.9486\n",
            "Epoch: 28, Accuracy: 0.9489\n",
            "Epoch: 29, Accuracy: 0.9489\n",
            "Epoch: 30, Accuracy: 0.949\n",
            "Epoch: 31, Accuracy: 0.9493\n",
            "Epoch: 32, Accuracy: 0.9498\n",
            "Epoch: 33, Accuracy: 0.9498\n",
            "Epoch: 34, Accuracy: 0.9497\n",
            "Epoch: 35, Accuracy: 0.9502\n",
            "Epoch: 36, Accuracy: 0.9506\n",
            "Epoch: 37, Accuracy: 0.9508\n",
            "Epoch: 38, Accuracy: 0.9508\n",
            "Epoch: 39, Accuracy: 0.9508\n",
            "Epoch: 40, Accuracy: 0.9502\n",
            "Epoch: 41, Accuracy: 0.9508\n",
            "Epoch: 42, Accuracy: 0.9504\n",
            "Epoch: 43, Accuracy: 0.9506\n",
            "Epoch: 44, Accuracy: 0.9507\n",
            "Epoch: 45, Accuracy: 0.9509\n",
            "Epoch: 46, Accuracy: 0.9511\n",
            "Epoch: 47, Accuracy: 0.9514\n",
            "Epoch: 48, Accuracy: 0.9515\n",
            "Epoch: 49, Accuracy: 0.9513\n",
            "CPU times: user 5min 50s, sys: 54.9 s, total: 6min 45s\n",
            "Wall time: 2min 1s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "network_with_checkpoints.SGD((x_train, y_train), epochs=50, mini_batch_size=100, eta=3., test_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.8548\n",
            "Epoch: 1, Accuracy: 0.8906\n",
            "Epoch: 2, Accuracy: 0.9056\n",
            "Epoch: 3, Accuracy: 0.9134\n",
            "Epoch: 4, Accuracy: 0.9197\n",
            "Epoch: 5, Accuracy: 0.9253\n",
            "Epoch: 6, Accuracy: 0.9277\n",
            "Epoch: 7, Accuracy: 0.9309\n",
            "Epoch: 8, Accuracy: 0.9341\n",
            "Epoch: 9, Accuracy: 0.9362\n",
            "Epoch: 10, Accuracy: 0.9371\n",
            "Epoch: 11, Accuracy: 0.9384\n",
            "Epoch: 12, Accuracy: 0.9399\n",
            "Epoch: 13, Accuracy: 0.941\n",
            "Epoch: 14, Accuracy: 0.9423\n",
            "Epoch: 15, Accuracy: 0.9435\n",
            "Epoch: 16, Accuracy: 0.9438\n",
            "Epoch: 17, Accuracy: 0.9441\n",
            "Epoch: 18, Accuracy: 0.9458\n",
            "Epoch: 19, Accuracy: 0.9465\n",
            "Epoch: 20, Accuracy: 0.9472\n",
            "Epoch: 21, Accuracy: 0.9475\n",
            "Epoch: 22, Accuracy: 0.948\n",
            "Epoch: 23, Accuracy: 0.9481\n",
            "Epoch: 24, Accuracy: 0.9486\n",
            "Epoch: 25, Accuracy: 0.9491\n",
            "Epoch: 26, Accuracy: 0.9491\n",
            "Epoch: 27, Accuracy: 0.9496\n",
            "Epoch: 28, Accuracy: 0.9504\n",
            "Epoch: 29, Accuracy: 0.9504\n",
            "Epoch: 30, Accuracy: 0.9507\n",
            "Epoch: 31, Accuracy: 0.9509\n",
            "Epoch: 32, Accuracy: 0.9512\n",
            "Epoch: 33, Accuracy: 0.9515\n",
            "Epoch: 34, Accuracy: 0.9516\n",
            "Epoch: 35, Accuracy: 0.9511\n",
            "Epoch: 36, Accuracy: 0.9511\n",
            "Epoch: 37, Accuracy: 0.9508\n",
            "Epoch: 38, Accuracy: 0.9506\n",
            "Epoch: 39, Accuracy: 0.9506\n",
            "Epoch: 40, Accuracy: 0.9511\n",
            "Epoch: 41, Accuracy: 0.951\n",
            "Epoch: 42, Accuracy: 0.9511\n",
            "Epoch: 43, Accuracy: 0.9507\n",
            "Epoch: 44, Accuracy: 0.9508\n",
            "Epoch: 45, Accuracy: 0.9512\n",
            "Epoch: 46, Accuracy: 0.951\n",
            "Epoch: 47, Accuracy: 0.951\n",
            "Epoch: 48, Accuracy: 0.9506\n",
            "Epoch: 49, Accuracy: 0.9505\n",
            "CPU times: user 5min 6s, sys: 50.3 s, total: 5min 57s\n",
            "Wall time: 1min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "network_no_checkpoints.SGD((x_train, y_train), epochs=50, mini_batch_size=100, eta=3., test_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ref 3. As we see the time difference is $6.45$ minutes vs $5.57$ minutes in favour of the 'standard' network, if we were to compare memory usage as a separate functions then $F_{standard}(a, g) = 2 * \\sum_{L = 1}^{N_L}(a_L + g_L)$\n",
        "and $F_{checkpoints} = \\frac{N_L}{k} + (k - 1) $ where k is checkpoint checking length. When it comes to the complexity, standard implementation only calculate one forward and backward pass, thus making around $N_L$ operations and because we recompute only once each layer then version with checkpoints makes only $2 \\cdot N_L$ operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
